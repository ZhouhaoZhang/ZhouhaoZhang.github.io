---
title: Home 
---
<!--meta name="viewport" content="width=2160px"--> 
<!-- ËÆæÁΩÆÂõ∫ÂÆöÂÆΩÂ∫¶‰∏∫1024pxÊàñ‰Ω†Â∏åÊúõÁöÑÊ°åÈù¢ÂÆΩÂ∫¶ -->



# Zhouhao Zhang

#### *Hi, welcome to my homepage!* ü§ó

## üëã About Me
[<img src="./images/photo.jpg" style="max-width:10%;min-width:40px;float:right;"/>](https://zhouhaozhang.github.io)
My name is Zhouhao Zhang, I'm currently a research intern at [Zhipu AI](https://www.zhipuai.cn/en/) X-lab under the guidance of [Dr. Tong Yang](https://scholar.google.com/citations?user=gxdjxrwAAAAJ&hl=en&oi=ao). My work focuses on intelligent robot research. 

Prior to this, I got my Bachelor's degree in Automation from [Beihang University](https://ev.buaa.edu.cn). I was honored with the National Scholarship and got national third place in the 2024 China University Robot Competition (ROBOCON).

## ü¶æ My Skills
* **Languages**: Chinese (Native), English (TOEFL: 93)
* **Programming Languages**: Python, C, C++
* **Robot Development Environments**: Linux, ROS, ROS2, Gazebo, PyBullet, MATLAB, SolidWorks
* **Development Libraries**: Pytorch, OpenCV, OpenAI Gym, PCL, Eigen
* **Robot Hardware**: LiDAR, Depth Camera, IMU, TOF
* **Embedded Systems**: NVIDIA Jetson, Raspberry Pi

## üèÖ My Awards
* **National Scholarship** | <span class="red">Top 3 out of 236</span>, The Ministry of Education of the People's Republic of China<div style="float: right;">Dec 2022</div>
* **First Prize, 23rd China University Robot Competition (ROBOCON)** | <span class="red">National Third Place</span><div style="float: right;">Jul 2024</div>
* **Second Prize, 23rd China University Robot Competition (RoboMaster)**<div style="float: right;">Apr 2024</div>
* **Second Prize, 22nd China University Robot Competition (ROBOCON)**<div style="float: right;">Jul 2023</div>
* **First Prize, 2022 Five Provinces of North China University Robot Competition**<div style="float: right;">Nov 2022</div>
* **First Prize, 2022 Beijing University Robot Competition**<div style="float: right;">Nov 2022</div>
* **Second Prize, China Intelligent Robot Fighting and Gaming Competition**<div style="float: right;">Mar 2023</div>
* **Third Prize, 38th National College Physics Competition**<div style="float: right;">Dec 2021</div>
* **Outstanding Graduate Award** | Beihang University<div style="float: right;">Jun 2024</div>
* **CATIC Scholarship** | <span class="red">Total 10 Places in Beihang University</span><div style="float: right;">Dec 2023</div>
* **First Prize, Scholarship in Discipline Competition** | Beihang University<div style="float: right;">Nov 2023</div>
* **Second Prize, Scholarship in Outstanding Social Work** | Beihang University<div style="float: right;">Nov 2023</div>
* **Outstanding Student Leader** | Beihang University<div style="float: right;">Nov 2023</div>
* **Top Prize, Learning Excellence Scholarship** | <span class="red">Top 3 out of 154</span>, Beihang University<div style="float: right;">Dec 2022</div>
* **Outstanding Student** | <span class="red">Top 2 out of 154</span>, Beihang University<div style="float: right;">Sep 2022</div>
* **Third Prize, 32nd FengRu Cup Competition & Yuyuan Robots Competition** | Beihang University<div style="float: right;">Jun 2022</div>

## üßëüèª‚Äçüíª Internship

<h3>
  Zhipu AI X-lab <div style="float: right;">Jun 2024 ‚Äì Present</div>
  <img src="./images/zhipu_logo.png" alt="Zhipu AI">
</h3>

I am conducting research in embodied AI, specifically mobile robots and their scene understanding.


<h3>
Skyforce Technology <img src="./images/skyforce_logo.png" alt="Skyforce"> <div style="float: right;">Jul 2023 ‚Äì Dec 2023</div>
</h3>
I interned at a startup, where I developed automatic keystone correction algorithms for projectors using both structured light and Time-of-Flight (TOF) technologies.

* [Auto Keystone Correction with Structured Light](./internship/auto%20keystone%20correction%20projector%20with%20camera/README.md): This project focused on using local homography and Gray code for calibration. The correction process involved triangulating the depth of key points and fitting the projection plane, while an accelerometer measured the direction of gravity. The homography matrix was computed by correlating key points on the wall with those on the projection screen, allowing for reconstructing the display area. The goal was to maximize the size and sharpness of the inner rectangle within an arbitrary convex projection. A national invention patent for this work is pending.

* [Auto Keystone Correction Projector with TOF](./internship/auto%20keystone%20correction%20projector%20with%20TOF/): This project utilized the VL53L5CX multi-point TOF sensor to detect the projection plane. Data stability was enhanced through filtering, and robustness was improved using the Random Sample Consensus (RANSAC) algorithm. The projection system of the ultra-short-throw projector was modeled using an equivalent ideal pinhole.
  <div class='pics_in_a_row'>
    <div style="flex:1.332812;">
      <img src='./images/projector.jpg'>
    </div>
    <div style="flex:1.035264;">
      <img src='./images/projector2.jpg'>
    </div>
    <div style="flex:1.757222;">
      <img src="./images/camera1.gif">
    </div>
    <div style="flex:1.777778;">
      <img src="./images/tof.gif">
    </div>
    <div style="flex:1.799440;">
      <img src="./images/calib.png">
    </div>
  </div>

## üõ†Ô∏è Projects

<h3>
  BR Robotics Team  <img src="./images/brlogo.png" alt="BR"> <div style="float: right;">Aug 2022 ‚Äì Jun 2024</div>
</h3>

I was a member of the BR Robotics Team for two years, representing Beihang University in the ROBOCON 2023, RoboMaster 2024, and ROBOCON 2024 competitions. As my skills developed, our team achieved increasingly impressive results. <span class="red">We got national third place in the 2024 China University Robot Competition (ROBOCON)</span>.

<div align="center">
    <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=358108413&bvid=BV1mX4y1Y7Pd&cid=1187445702&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 45%; aspect-ratio: 16 / 9;"></iframe>
    <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1302854537&bvid=BV1zM4m197an&cid=1495784623&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 45%; aspect-ratio: 16 / 9;"></iframe>
    <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=112899104180386&bvid=BV1Kav9e5EfW&cid=500001637420342&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 45%; aspect-ratio: 16 / 9;"></iframe>
    <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1006197069&bvid=BV1qx4y1x7nz&cid=1612987246&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 45%; aspect-ratio: 16 / 9;"></iframe>
</div>

* [Auto-shoot Algorithm for Racing Robot in CURC ROBOCON 2023](./robotics%20team%20of%20BUAA/ROBOCON2023/): This algorithm combines LiDAR, wheel odometry, and IMU data for target localization, while a deep learning model identifies and tracks targets. The localization data refines the shooting accuracy by calculating angular deviations, which are then transmitted to the motor driver. The performance in the 2023 CURC ROBOCON competition validated the algorithm‚Äôs precision and robustness.   
  <div class='pics_in_a_row'>
    <div style="flex:1.848592;">
      <img src='./images/RC2023_field.jpg'>
    </div>
    <div style="flex:1.777778;">
      <img src='./images/robocon.jpg'>
    </div>
    <div style="flex:1.503012;">
      <img src='./images/elephant.jpg'>
    </div>
    <div style="flex:1.170132;">
      <img src='./images/rabbit.jpg'>
    </div>
  </div>

  <div class='pics_in_a_row'>
    <div style="flex:1.775000;">
      <img src='./images/path.gif'>
    </div>
    <div style="flex:1.775000;">
      <img src='./images/aim.gif'>
    </div>
    <div style="flex:1.775000;">
      <img src='./images/grasp.gif'>
    </div>
  </div>

* [Decision-making algorithm for autonomous robots for ROBOCON 2024](./robotics%20team%20of%20BUAA/ROBOCON2024/): This competition required the development of an advanced decision-making algorithm to win by strategically occupying zones in the competition. A minimax search algorithm with alpha-beta pruning was implemented. Finite state machine was used to manage the robot's behavior.
  <div class='pics_in_a_row'>
    <div style="flex:1.261580;">
      <img src='./images/RC2024_field.jpg'>
    </div>
    <div style="flex:1.524390;">
      <img src='./images/RC2024_1.gif'>
    </div>
    <div style="flex:1.524390;">
      <img src='./images/RC2024_2.gif'>
    </div>
    <div style="flex:1.777778;">
      <img src='./images/rc24v.gif'>
    </div>
  </div>

* Target trajectory analysis with stereo camera : To optimize detection efficiency, a sliding window technique was introduced, reducing computational load by utilizing previous frame recognition data. Triangulation principles calculated 3D coordinates of the target, while Kalman filtering ensured data stability and robustness.
  <div class='pics_in_a_row'>
    <div style="flex:3.380435;">
      <img src='./images/stereo.gif'>
    </div>
    <div style="flex:3.380282;">
      <img src='./images/bottle.gif'>
    </div>
  </div>

* [Team entry test](./robotics%20team%20of%20BUAA/training/README.md): This test, designed for new team members, required camera pose estimation using the Perspective-n-Point (PNP) algorithm. By detecting corner points and establishing correspondences between 2D images and known 3D coordinates, the camera‚Äôs trajectory was calculated.
  <div class='pics_in_a_row'>
    <div style="flex:1.781991;">
      <img src='./images/pnp.gif'>
    </div>
    <div style="flex:1.006452;">
      <img src='./images/pnp2.gif'>
    </div>
  </div>

* Team trainning: I provided technical training to new members, covering essential topics such as image processing, 3D vision, and introductory courses on Linux and ROS. I contributed to the construction of the first RoboMaster team of Beihang University, I led the team got the Second Prize of 23rd RoboMaster.
  <div class='pics_in_a_row'>   
    <div style="flex:1.891482;">
      <img src='./images/trainning.jpg'>
    </div>
    <div style="flex:1.503012;">
      <img src='./images/RM2.jpg'>
    </div>
    <div style="flex:1.777778;">
      <img src='./images/rmv.gif'>
    </div>
  </div>

<h3>
Biomechanics and Soft Robotics Lab <img src="./images/buaalogo.png" alt="buaa"> <div style="float: right;">Apr 2023 ‚Äì Oct 2023</div>
</h3>

I joined Biomechanics and Soft Robotics Lab led by [Prof. Li Wen](https://scholar.google.com.hk/citations?user=u86Pm9UAAAAJ&hl=en). Under the guidence of [Dr. Lei Li](https://scholar.google.com/citations?user=KId65yQAAAAJ&hl=en), we developed [An Aerial‚ÄìAquatic Hitchhiking Robot with Remora-Inspired Tactile Sensors and Thrust Vectoring Units](https://onlinelibrary.wiley.com/doi/10.1002/aisy.202300381). My contributions included debugging flight control systems, assisting in various experiments, and deploying SLAM and autonomous navigation algorithms for the next-generation robots. Equipped with thrust vectoring units and a remora-inspired tactile suction cup, this UAV is capable of transitioning between water and air, performing agile underwater maneuvers, and conserving energy by hitchhiking on surfaces with varying roughness.
<div align="center">
  <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=619741574&bvid=BV1g84y1d7YH&cid=1301361611&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 33%; aspect-ratio: 16 / 9;"></iframe>
  <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=865438757&bvid=BV1a54y1u77Y&cid=1035540328&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 33%; aspect-ratio: 16 / 9;"></iframe>
  <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=733647812&bvid=BV1hD4y1a78V&cid=921457829&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 33%; aspect-ratio: 16 / 9;"></iframe>
</div>

  <div class='pics_in_a_row'>
    <div style="flex:1.336735;">
      <img src='./images/drone1.jpg'>
    </div>
    <div style="flex:1.228840;">
      <img src='./images/drone2.jpg'>
    </div>
    <div style="flex:1.776892;">
      <img src='./images/underwater.gif'>
    </div>
  </div>

  <div class='pics_in_a_row'>
    <div style="flex:1.337398;">
      <img src='./images/rtabmap.gif'>
    </div>
    <div style="flex:1.730964;">
      <img src='./images/rtabmap2.gif'>
    </div>
    <div style="flex:1.777778;">
      <img src='./images/fastlio2.gif'>
    </div>
  </div>

<h3>
  Graduation Project <img src="./images/cuhklogo.jpg" alt="CUHK"> <div style="float: right;">Dec 2023 ‚Äì Jun 2024</div>
</h3>

I completed my graduation project under the remote guidance of [Prof. Qi Dou](https://scholar.google.com/citations?user=iHh7IJQAAAAJ&hl=en&oi=ao) at the Chinese University of Hong Kong. My project focused on task planning for a 7DOF da Vinci Surgical Robot using demonstration-guided reinforcement learning and policy chains. Additionally, I worked on surgical robot visual manipulation policy learning through world model-based reinforcement learning and developed high-level task planning strategies using large language models and behavior trees.

  <div class='pics_in_a_row'>
    <div style="flex:1.430769;">
      <img src='./images/davinci.png'>
    </div>
    <div style="flex:1.286432;">
      <img src='./images/pegboard2.gif'>
    </div>
    <div style="flex:1.333333;">
      <img src='./images/pegboard.gif'>
    </div>
    <div style="flex:1.333333;">
      <img src='./images/pegtransfer.gif'>
    </div>
    <div style="flex:1.500000;">
      <img src='./images/dreamer.gif'>
    </div>
  </div>

<h3>
  RedBird Challenge Camp at HKUST(GZ) <img src="./images/hkustgzlogo.png" alt="hkust gz"> <div style="float: right;">Jun 2024</div>
</h3>

To enhance factory environment detection, our group designed a mobile intelligent detection robot equipped with multiple sensors. I served as the algorithm engineer, implemented 2D SLAM using the Cartographer algorithm, and developed autonomous navigation and dynamic obstacle avoidance systems using the ROS Navigation stack. I built a factory simulation environment in Gazebo to demonstrate our design.

  <div class='pics_in_a_row'>
    <div style="flex:1.600000;">
      <img src='./images/explore.gif'>
    </div>
    <div style="flex:1.600000;">
      <img src='./images/navs.gif'>
    </div>
    <div style="flex:1.600000;">
      <img src='./images/navd.gif'>
    </div>
  </div>

<h3>
  AI Program in NUS <img src="./images/nuslogo.jpg" alt="nus"> <div style="float: right;">Aug 2023</div>
</h3>

During my participation in the NUS Artificial Intelligence and Machine Learning Summer Program, I led my team to develop an innovative [Seq2Seq model using LSTM for population forecasting](./AI%20program%20in%20NUS/), going beyond the basic regression model initially required. Our work earned us winning team, and we received high praise from [Prof. Mehul Motani](https://scholar.google.com/citations?user=Bm9BwEQAAAAJ&hl=en&oi=ao).
    <h6>
      <img src="images/NUS.jpg" height=200px/>
      <img src="images/NUS2.jpg" height=200px/>
    </h6>

## üìö Course Projects
I take each experiment seriously, cherish these practical opportunities, and always exceed the teacher's tasks. This seriousness is also reflected in my grades.

* PointNet/PointNet++ point cloud segmentation: I led my team in studying the architectures of PointNet and PointNet++. By leveraging a common backbone with different heads, we successfully executed both classification and segmentation tasks on point clouds. I further explored the T-Net module, experimenting with structural modifications like residual connections to observe different performance outcomes.

  <div class='pics_in_a_row'>
    <div style="flex:1.988864;">
      <img src='./images/pn3.png'>
    </div>
    <div style="flex:2.308642;">
      <img src='./images/pn1.jpg'>
    </div>
    <div style="flex:1.992958;">
      <img src='./images/pn2.jpg'>
    </div>
  </div>

* [Experiments on Eight-Puzzle graph search algorithms](./in-class%20experiments/8dig): Using BFS, DFS, and A* algorithms, I applied graph search strategies to solve the eight-puzzle problem. Through this experiment, we analyzed the differences in search strategies and performance.


  <div class='pics_in_a_row_small'>
    <div style="flex:1.3307;">
      <img src='images/bfs.jpg'>
    </div>
    <div style="flex:1.3362;">
      <img src='images/astar.jpg'>
    </div>
  </div>


* [Comparison experiments between CNN and Dense](./in-class%20experiments/Comparison%20experiments%20between%20CNN%20and%20Dense/README.md):  Inspired by Prof. Li Mu‚Äôs Dive into Deep Learning, I constructed various classic neural networks for the MNIST and Fashion MNIST datasets, comparing their performances. I also visualized layer-wise results for LeNet to better understand the mechanics of CNNs. Additionally, I tested LeNet on handwritten Arabic characters.
  <div class='pics_in_a_row'>
    <div style="flex:1.0654;">
      <img src='images/cnn.png'>
    </div>
    <div style="flex:1.9753;">
      <img src='images/cnn2.png'>
    </div>
    <div style="flex:2.1366;">
      <img src='images/arb.jpg'>
    </div>
  </div>


* [Experiments on Medical Image segmentation (Liver)](./in-class%20experiments/Experiments%20on%20Medical%20Image%20segmentation/liver/README.md), [(Retinal vessels)](./in-class%20experiments/Experiments%20on%20Medical%20Image%20segmentation/Retinal%20vessels/README.md) In this experiment, I reproduced U-Net using PyTorch and optimized its hyperparameters to train effectively on a small dataset. While working on liver CT segmentation, I tackled the challenge of uneven data distribution by applying normalization, leading to successful results.

  <div class='pics_in_a_row'>
    <div style="flex:2.637209;">
      <img src='./images/liver1.jpg'>
    </div>
    <div style="flex:2.465632;">
      <img src='./images/liver2.jpg'>
    </div>
    <div style="flex:2.652381;">
      <img src='./images/retinalvessels2.jpg'>
    </div>
    <div style="flex:2.546911;">
      <img src='./images/retinalvessels1.jpg'>
    </div>
  </div>

* [EEG-based Motor Imagery Classification](./in-class%20experiments/EEG-based%20Motor%20Imagery%20Classification/README.md): I implemented the EEGNet network to classify motor imagery using EEG signals. Through this experiment, I gained a deep understanding of convolution techniques like group, depth-wise, and point-wise convolutions. My model achieved top rankings in both binary and four-class classification tasks within the class.
    
  <div class='pics_in_a_row'>
    <div style="flex:2.064655;">
      <img src='./images/eeg1.jpg'>
    </div>
    <div style="flex:1.789474;">
      <img src='./images/eeg2.png'>
    </div>
  </div>

* [Robot path planning experiments](./in-class%20experiments/Robot%20path%20planning%20experiments/README.md) I explored the differences in various heuristic functions in robot path planning tasks and summarized my findings in an experimental report.
  <div align="center">
    <iframe src="//player.bilibili.com/player.html?isOutside=true&aid=953162139&bvid=BV1Us4y1g7rq&cid=1116087360&p=1" scrolling="no" frameborder="no" width=440 style="width: 45%; aspect-ratio: 16 / 9;"></iframe>
  </div>

* [GMM Built by Expectation-Maximization Algorithm](./in-class%20experiments/GMM%20Built%20by%20EM%20Algorithm/) Given a dataset, I used the Expectation-Maximization algorithm to fit a Gaussian mixture model to it and determine the parameters of the distribution.

    <h6>
      <img src="./images/gmm.jpg"/>
    </h6>

## üéûÔ∏è Gallery

* I'm very lucky to have had a great time with you.

  <div class='pics_in_a_row'>
    <div style="flex:1.5;">
      <img src='./images/groupphoto1.jpg'>
    </div>
    <div style="flex:1.5;">
      <img src='./images/groupphoto2.jpg'>
    </div>
    <div style="flex:1.5;">
      <img src='./images/groupphoto3.jpg'>
    </div>
  </div>

* The following images were taken on medium format film.

  <div class='pics_in_a_row'>
    <div style="flex:0.819203;">
      <img src='./gallery/1.jpeg'>
    </div>
    <div style="flex:1.221022;">
      <img src='./gallery/8.jpg'>
    </div>
    <div style="flex:1.220963;">
      <img src='./gallery/2.jpg'>
    </div>
    <div style="flex:1.220963;">
      <img src='./gallery/3.jpg'>
    </div>
    <div style="flex:1.220963;">
      <img src='./gallery/4.jpg'>
    </div>
    <div style="flex:1.220994;">
      <img src='./gallery/6.jpeg'>
    </div>
    <div style="flex:1.220963;">
      <img src='./gallery/7.jpg'>
    </div>
    <div style="flex:1.220963;">
      <img src='./gallery/9.jpg'>
    </div>
    <div style="flex:1.231539;">
      <img src='./gallery/5.jpeg'>
    </div>
    <div style="flex:1.000000;">
      <img src='./gallery/14.jpeg'>
    </div>
  </div>


<style>
.row {
  white-space: nowrap;
  display: inline-block;
  opacity: 0;
  font-size: 0;
}

.row img {
  height: 100%;
  display: inline-block;
}
</style>
  

<script>
function sizeRow(row) {
  // Ëé∑Âèñ data-required-width ÊàñËÄÖÁà∂Á∫ßÂÖÉÁ¥†ÁöÑÂÆΩÂ∫¶ÔºàÂçï‰Ωç‰∏∫ÂÉèÁ¥†Ôºâ
  let requiredWidth = row.getAttribute('data-required-width') || row.parentElement.offsetWidth + 'px';
  
  // Âä®ÊÄÅËÆæÁΩÆCSSÂèòÈáè
  row.style.setProperty('--requiredWidth', requiredWidth);
  
  // ‰∏¥Êó∂È´òÂ∫¶ÔºåËÆ°ÁÆóÊØî‰æãÁî®
  row.style.height = '100px'; 
  
  // Ë∞ÉÊï¥È´òÂ∫¶‰ª•ÈÄÇÂ∫îÂÆΩÂ∫¶ÊØî‰æã
  row.style.height = 'calc(var(--requiredWidth) * ' + 100 / row.offsetWidth + ')';
  
  // ÊòæÁ§∫ÂÜÖÂÆπ
  row.style.opacity = 1;
}

function resizeAllRows() {
  let rows = document.querySelectorAll('.row');
  rows.forEach(sizeRow);
}

window.onload = resizeAllRows;
window.onresize = resizeAllRows;
</script>